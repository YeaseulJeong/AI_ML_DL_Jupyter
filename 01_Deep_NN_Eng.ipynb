{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Process \n",
    "    1. Generate Linear NN Model_ linear = nn.Linear(X, Y)\n",
    "    2. Define Loss function, Optimiser in advance\n",
    "    3. Prediction _ pred = linear(X)\n",
    "    4. Loss function\n",
    "    5. Backward Propagation()\n",
    "    6. Training_ optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch     # Pythorch framework ... called torch\n",
    "import torchvision   # Library used in Image Processing (Visiong)in torch\n",
    "import torch.nn as nn   # For Neural Network \n",
    "import numpy as np  \n",
    "import torchvision.transforms as transforms   # For Data Augentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4490e-39, 1.0102e-38, 9.0919e-39],\n",
      "        [1.0102e-38, 8.9082e-39, 8.4489e-39]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor(2,3)\n",
    "# Same as =np.array([])\n",
    "# Creating (2,3) shape of dataset using torch\n",
    "# Dataset is initialised randomly\n",
    "# Manually set the dataset like X=torch.Tensor([[1,2,3], [4,5,6]]), target = torch.tensor([3.0, 4.0]) if not Random\n",
    "\n",
    "print(X)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "X=torch.Tensor([[1,2,3], [4,5,6]])\n",
    "print(X)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 9.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(data =[2.0,3.0], requires_grad=True)   \n",
    "# requires_grad=True  is to set value of the gradient (slope)\n",
    "# If it is set to False, can't do back propagaction (cuz don't have gradient to differentiate)\n",
    "\n",
    "target = torch.tensor([3.0, 4.0])  \n",
    "\n",
    "y=x**2\n",
    "\n",
    "pred = 2*x + 3    # Predicted values\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(9., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(torch.abs(pred-target))    #(11-3)+(21-4)= 25  or (7-3)+(9-4)=9\n",
    "print(\"loss\", loss)\n",
    "\n",
    "# Calculate Loss using Predicted values and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 12.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()   # Partial derivate goes on based on the loss value\n",
    "print(x.grad)     # Output: Differentiated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating NeuralNet Using Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(2., requires_grad=True)\n",
      "tensor(3., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Making Hypothes  ex)2x+3 \n",
    "# requires_grad=True  for deffierential\n",
    "\n",
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3., requires_grad = True)\n",
    "\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "y = w*x +b   \n",
    "# Making Linear Function using x, w, b created above\n",
    "\n",
    "y.backward()   \n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# .grad shows the result of the .backward() = differential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Two-Dimensional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8540, -0.4819, -1.1634],\n",
      "        [ 0.6633, -1.7154,  0.0080],\n",
      "        [ 1.4841, -1.2872,  0.9404],\n",
      "        [-1.7820, -0.3503, -0.8884],\n",
      "        [-0.0420,  0.8181, -0.3515],\n",
      "        [-2.1385, -1.0630, -0.9695],\n",
      "        [-0.9215,  1.2832, -1.8755],\n",
      "        [ 0.1687, -0.5799, -1.1411],\n",
      "        [-2.5008,  0.0484,  1.0711],\n",
      "        [-0.6472,  1.0035,  0.9775]])\n",
      "tensor([[-0.9164,  0.3917],\n",
      "        [-0.5226, -1.0022],\n",
      "        [ 0.5995,  0.0933],\n",
      "        [-1.0351,  0.8484],\n",
      "        [ 0.8683,  0.4521],\n",
      "        [ 1.4030, -1.2594],\n",
      "        [-0.0867, -0.0369],\n",
      "        [ 0.9726,  0.4877],\n",
      "        [-0.3979, -0.8892],\n",
      "        [ 0.6025, -1.0498]])\n",
      "w:  Parameter containing:\n",
      "tensor([[-0.0501, -0.1610,  0.1946],\n",
      "        [-0.2623,  0.5052, -0.1571]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.4628, -0.2250], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(10,3)  \n",
    "# Creating two dimensional dataset for the input\n",
    "\n",
    "y=torch.randn(10,2)    \n",
    "# Creating two dimensional dataset for the target\n",
    "# randn() randomly selects numbers that follows Normal Distribution \n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "linear = nn.Linear(3,2)     \n",
    "# Creating linear cliassifier (=linear)\n",
    "# Input is 10 * 3 and Output is 10 * 2  >>>  Neural Network between the input and the output must be 3 * 2 \n",
    "\n",
    "# In the FIRST run, RANDOMLY created Weights and Bias in Neural Network\n",
    "print('w: ', linear.weight)   \n",
    "# Checking Weights \n",
    "# Should be (3,2) shape but with no reason the output is trasposed to (2,3)\n",
    "\n",
    "print('b: ', linear.bias)     \n",
    "# Should be (10,2) because these will be added to y values which has the shape of (10,2) \n",
    "# But bias is all THE SAME for all the outputs (predictions)  so showing ONLY ONE  (1,2) the rest are skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network >> Predict >> Loss value >> Loss Function >> Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000002D418AB34A0>\n"
     ]
    }
   ],
   "source": [
    "print(linear.parameters())\n",
    "# .parameters() contains the subjects that are being trained (Weights and Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss... before step back-propagation :  0.9029954671859741\n"
     ]
    }
   ],
   "source": [
    "# Loss function(mse, cross entropy, ...) and optimiser(adam, sgd, rmsprop ...) \n",
    "# Should be defined in the beginning and CALLit when in use\n",
    "\n",
    "loss_function = nn.MSELoss()   \n",
    "# Defining Loss Function \n",
    "\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)   \n",
    "# Defining Optimiser\n",
    "# Designating the Learning Rate\n",
    "# linear.parameters() has W and b \n",
    "\n",
    "pred = linear(x)\n",
    "loss = loss_function(pred , y)\n",
    "print('loss... before step back-propagation : ', loss.item())\n",
    "# .item() : Chaning numpy to tenrsor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw :  tensor([[-0.3379, -0.5209,  0.0847],\n",
      "        [-1.8608,  0.9451,  0.1630]])\n",
      "dL/db :  tensor([0.6109, 0.0626])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "# Training hasn't begun only going through backpropagation (partial derivative)\n",
    "# Finding how much Weights and Bias should change \n",
    "\n",
    "print('dL/dw : ', linear.weight.grad)\n",
    "print('dL/db : ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()   \n",
    "# Training begins based on the backpropagation results\n",
    "# Reapeating this == Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss... after step back-propagation :  0.8519375920295715\n"
     ]
    }
   ],
   "source": [
    "pred = linear(x)\n",
    "loss = loss_function(pred, y)\n",
    "\n",
    "print('loss... after step back-propagation : ', loss.item())\n",
    "# Loss value after the optimising using back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Process \n",
    "    1. Generate Linear NN Model_ linear = nn.Linear(X, Y)\n",
    "    2. Define Loss function, Optimiser in advance\n",
    "    3. Prediction _ pred = linear(X)\n",
    "    4. Loss function\n",
    "    5. Backward Propagation()\n",
    "    6. Training_ optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
